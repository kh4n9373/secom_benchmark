#!/bin/bash
# SeCom Benchmark Setup Script
# Install dependencies and download datasets for SeCom benchmark

set -e  # Exit on error

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

echo "============================================================"
echo "üîß SeCom Benchmark Setup"
echo "============================================================"
echo ""

# 1. Check Python version
echo "‚ñ∂ Checking Python..."
PYTHON_VERSION=$(python3 --version 2>&1 | awk '{print $2}' | cut -d. -f1,2)
echo "   Python version: $PYTHON_VERSION"

if [[ $(echo "$PYTHON_VERSION < 3.8" | bc -l) -eq 1 ]]; then
    echo "‚ùå Python 3.8+ required (found $PYTHON_VERSION)"
    exit 1
fi
echo "‚úÖ Python version OK"
echo ""

# 2. Set TMPDIR to avoid disk space issues
echo "‚ñ∂ Setting TMPDIR to avoid disk space issues..."
export TMPDIR="$SCRIPT_DIR/.tmp"
export TEMP=$TMPDIR
export TMP=$TMPDIR
mkdir -p "$TMPDIR"
echo "‚úÖ TMPDIR set to $TMPDIR"
echo ""

# 3. Install SeCom package
echo "‚ñ∂ Installing SeCom package..."
if [ -f "setup.py" ]; then
    pip install -e . --quiet
    echo "‚úÖ SeCom package installed"
else
    echo "‚ùå setup.py not found"
    exit 1
fi
echo ""

# 4. Install additional dependencies
echo "‚ñ∂ Installing additional dependencies..."
echo "   This may take a few minutes..."
echo ""

# Install llmlingua for compression
pip install llmlingua --quiet
echo "‚úÖ llmlingua installed"

# Install langchain and related packages
pip install langchain langchain-community --quiet
echo "‚úÖ langchain installed"

# Install vector store and embeddings
pip install chromadb sentence-transformers faiss-cpu --quiet
echo "‚úÖ chromadb, sentence-transformers, faiss installed"

# Install evaluation packages
pip install rouge-score sacrebleu bert-score nltk --quiet
echo "‚úÖ evaluation packages installed"

# Install other utilities
pip install python-dotenv tqdm omegaconf tiktoken --quiet
echo "‚úÖ utility packages installed"

echo ""

# 5. Download NLTK data
echo "‚ñ∂ Downloading NLTK data..."
python3 -c "import nltk; nltk.download('punkt', quiet=True); nltk.download('punkt_tab', quiet=True)" 2>/dev/null || true
echo "‚úÖ NLTK data downloaded"
echo ""

# 6. Check/create data symlink or download dataset
echo "‚ñ∂ Checking dataset..."
if [ -L "data" ] || [ -d "data" ]; then
    echo "‚úÖ Data directory exists"
    
    # Check for specific datasets
    LOCOMO_FILE="data/locomo/processed_data/locomo_processed_data.json"
    LONGMEMEVAL_FILE="data/locomo/processed_data/longmemeval_processed_data.json"
    
    if [ -f "$LOCOMO_FILE" ]; then
        echo "‚úÖ LoCoMo dataset found"
    else
        echo "‚ö†Ô∏è  LoCoMo dataset not found at $LOCOMO_FILE"
    fi
    
    if [ -f "$LONGMEMEVAL_FILE" ]; then
        echo "‚úÖ LongMemEval dataset found"
    else
        echo "‚ö†Ô∏è  LongMemEval dataset not found at $LONGMEMEVAL_FILE"
    fi
else
    echo "üì• Downloading dataset from HuggingFace..."
    mkdir -p data
    
    python3 <<'EOF'
from huggingface_hub import snapshot_download
import os

try:
    snapshot_download(
        repo_id="KhangPTT373/locomo",
        local_dir="data/locomo",
        repo_type="dataset"
    )
    print("‚úÖ Dataset downloaded successfully!")
except Exception as e:
    print(f"‚ùå Failed to download dataset: {e}")
    print("   You can manually create a symlink to shared data:")
    print("   ln -s ../mem0/data data")
    exit(1)
EOF
    
    if [ $? -ne 0 ]; then
        exit 1
    fi
fi
echo ""

# 7. Create necessary directories
echo "‚ñ∂ Creating directories..."
mkdir -p worker_logs
mkdir -p benchmark_results
mkdir -p test_results
echo "‚úÖ Directories created"
echo ""

# 8. Verify installation
echo "‚ñ∂ Verifying installation..."
python3 -c "from secom import SeCom; print('‚úÖ secom')" 2>/dev/null || echo "‚ö†Ô∏è  secom import failed"
python3 -c "from llmlingua import PromptCompressor; print('‚úÖ llmlingua')" 2>/dev/null || echo "‚ö†Ô∏è  llmlingua import failed"
python3 -c "import chromadb; print('‚úÖ chromadb')" 2>/dev/null || echo "‚ö†Ô∏è  chromadb not found"
python3 -c "import langchain; print('‚úÖ langchain')" 2>/dev/null || echo "‚ö†Ô∏è  langchain not found"
python3 -c "import sentence_transformers; print('‚úÖ sentence_transformers')" 2>/dev/null || echo "‚ö†Ô∏è  sentence_transformers not found"
python3 -c "import rouge_score; print('‚úÖ rouge_score')" 2>/dev/null || echo "‚ö†Ô∏è  rouge_score not found"
python3 -c "import bert_score; print('‚úÖ bert_score')" 2>/dev/null || echo "‚ö†Ô∏è  bert_score not found"
echo ""

# 9. Summary
echo "============================================================"
echo "‚úÖ Setup completed!"
echo "============================================================"
echo ""
echo "üìã What was done:"
echo "  ‚úì SeCom package installed"
echo "  ‚úì LLMLingua for compression installed"
echo "  ‚úì Dependencies installed (langchain, chromadb, etc.)"
echo "  ‚úì Dataset downloaded/verified"
echo "  ‚úì Directories created"
echo ""
echo "üìù Next steps:"
echo ""
echo "  1. (Required) Start self-hosted LLM server:"
echo "     vllm serve Qwen/Qwen3-8B --port 8001"
echo ""
echo "  2. Run quick test:"
echo "     ./quick_test.sh (if available)"
echo ""
echo "  3. Or run full benchmark:"
echo "     ./full_benchmark_locomo.sh"
echo "     ./full_benchmark_longmemeval.sh"
echo ""
echo "üí° Tips:"
echo "  - SeCom uses self-hosted LLM for segmentation (no OpenAI key needed)"
echo "  - Edit *.sh files to change LLM model/server settings"
echo "  - Check worker_logs/ if parallel indexing fails"
echo "  - Results saved to benchmark_results/"
echo ""
